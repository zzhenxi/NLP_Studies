{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7lMU2WDtbQy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzS4Kux3tbQ0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qynSGk1tbQ1"
      },
      "source": [
        "# Pytorch DataLoader\n",
        "Pytorch는 데이터를 전처리하고 배치화할 수 있는 클래스를 제공한다.    \n",
        "`Dataset` 클래스는 데이터를 **전처리**하고 dictionary 또는 list 타입으로 변경할 수 있다.   \n",
        "`DataLoader` 클래스는 데이터 **1. 셔플 2. 배치화 3. 멀티 프로세스** 기능을 제공한다. \n",
        "\n",
        "[OFFICAL DOCUMENT](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
        "\n",
        "# Table of Contents\n",
        "- [Dataset](#Dataset)\n",
        "- [Dataloader](#DataLoader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5LnY3M2tbQ2"
      },
      "source": [
        "## Dataset\n",
        "- 모든 custom dataset 클래스는 `Dataset()` 클래스를 상속받아야 함.\n",
        "- `__getitem__()`와 `__len__()` 메소드를 반드시 오버라이딩해야 함. \n",
        "- `DataLoader` 클래스가 배치를 만들 때 `Dataset` 인스턴스의 `__getitem__()` 메소드를 사용해 데이터에 접근함\n",
        "- 해당 Dataset 클래스는 string sequence 데이터를 **tokenize** & **tensorize**한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EqI5WCTXtbQ2"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import Iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQvCa86PtbQ2"
      },
      "outputs": [],
      "source": [
        "trainer_iter = AG_NEWS(split = 'train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBmycbhvnVrK"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    trainer_iter[0]\n",
        "except NotImplementedError:\n",
        "    print(f\"__getitem__() function not implemented.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BHquGVw3RTD"
      },
      "outputs": [],
      "source": [
        "next(trainer_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ0s2hgatbQ3"
      },
      "outputs": [],
      "source": [
        "class Custom_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, data: Iterator):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "        self.target = []\n",
        "        self.text = []\n",
        "        for target, text in data:\n",
        "            self.target.append(target)\n",
        "            self.text.append(text)\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.target)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # encode\n",
        "        token_ids = self.tokenizer.encode(\n",
        "        text = self.text[index],\n",
        "        truncation = True,\n",
        "        )\n",
        "        \n",
        "        # tensorize\n",
        "        return torch.tensor(token_ids), torch.tensor([self.target[index]])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIwzIDNKl8MZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = Custom_Dataset(trainer_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG5KwzljmGxZ"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPn5C8B9n7Pc"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIl5vcsvm5oB"
      },
      "outputs": [],
      "source": [
        "# decode to see original text\n",
        "train_dataset.tokenizer.decode(train_dataset[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhgHYusAtbQ3"
      },
      "source": [
        "## Dataloader\n",
        "- `dataset`\n",
        "    - **map-style** dataset\n",
        "    (`Dataset`)\n",
        "    - iterable style dataset\n",
        "      - `__iter__()`\n",
        "- `batch_size` \n",
        "  - int\n",
        "- `shuffle`\n",
        "  - bool\n",
        "- `sampler`\n",
        "  - data index 이터레이터\n",
        "- `collate_fn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOh5gczRtbQ4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Week2_3_dataloader.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}